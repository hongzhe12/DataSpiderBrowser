# spider.py

import re
import time
from typing import List, Dict, Any

import requests
from bs4 import BeautifulSoup

from crawlers.base_spider import BaseSpider  # å‡è®¾ BaseSpider åœ¨ crawlers/base_spider.py
from service.storage import cookie


class JdOrderSpider(BaseSpider):
    """
    äº¬ä¸œè®¢å•çˆ¬è™«
    åŠŸèƒ½ï¼šç™»å½•åçˆ¬å–ç”¨æˆ·çš„è®¢å•åˆ—è¡¨ï¼Œæå–è®¢å•ä¿¡æ¯å¹¶ä¿å­˜ä¸ºè¡¨æ ¼æ•°æ®ã€‚
    """

    def get_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿‡æ»¤æ‰ None æˆ–ç©ºå­—å…¸ {}"""
        return data

    def __init__(self, cookies: Dict[str, str], start_page: int = 1, end_page: int = None):
        """
        åˆå§‹åŒ–çˆ¬è™«ã€‚

        Args:
            cookies (Dict[str, str]): ç™»å½•äº¬ä¸œåçš„ Cookie å­—å…¸ã€‚
            start_page (int): èµ·å§‹é¡µç ã€‚
            end_page (int): ç»“æŸé¡µç ï¼ŒNone è¡¨ç¤ºçˆ¬å–åˆ°æ— æ•°æ®ä¸ºæ­¢ã€‚
        """
        self.cookies = cookies
        self.start_page = start_page
        self.end_page = end_page
        self.current_page = start_page

        # äº¬ä¸œè®¢å•åˆ—è¡¨ URL
        self.base_url = "https://order.jd.com/center/list.action"

        # è¯·æ±‚å¤´
        self.headers = {
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "accept-language": "zh-CN,zh;q=0.9,en;q=0.8",
            "cache-control": "no-cache",
            "pragma": "no-cache",
            "sec-ch-ua": '"Google Chrome";v="141", "Not?A_Brand";v="8", "Chromium";v="141"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "same-origin",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36",
        }

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            name="jd_order_spider",
            start_urls=[f"{self.base_url}?page={start_page}"],
            delay=1.5  # é¿å…è¯·æ±‚è¿‡å¿«
        )

    def start_requests(self) -> List[str]:
        """ç”Ÿæˆèµ·å§‹è¯·æ±‚ URL åˆ—è¡¨ï¼ˆä» start_page å¼€å§‹ï¼‰"""
        if self.end_page and self.end_page < self.start_page:
            return []
        # æˆ‘ä»¬ä¸åœ¨æ­¤å¤„ç”Ÿæˆæ‰€æœ‰é¡µç ï¼Œè€Œæ˜¯åœ¨ crawl è¿‡ç¨‹ä¸­åŠ¨æ€åˆ¤æ–­
        return [f"{self.base_url}?page={self.start_page}"]

    def parse(self, response: requests.Response, **kwargs) -> List[Dict[str, Any]]:
        """
        è§£æäº¬ä¸œè®¢å•é¡µé¢ HTMLï¼Œæå–è®¢å•ä¿¡æ¯ã€‚
        æ­¤æ–¹æ³•ç›´æ¥å¤ç”¨äº† spider1.py ä¸­çš„æ ¸å¿ƒè§£æé€»è¾‘ã€‚
        """
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            orders = []

            # æŸ¥æ‰¾æ‰€æœ‰è®¢å•çš„ tbody å…ƒç´ 
            order_tbodies = soup.find_all('tbody', id=re.compile(r'^tb-'))

            for tbody in order_tbodies:
                order = self._extract_order_info(tbody)
                if order:
                    orders.append(order)

            return orders

        except Exception as e:
            print(f"è§£æé¡µé¢æ—¶å‡ºé”™: {e}")
            return []

    def _extract_order_info(self, tbody) -> Dict[str, Any]:
        """ä»å•ä¸ªè®¢å• tbody ä¸­æå–ä¿¡æ¯ï¼ˆç§æœ‰æ–¹æ³•ï¼‰"""
        try:
            order = {}

            tr_th = tbody.find('tr', class_='tr-th')
            if not tr_th:
                return {}

            # è®¢å•å·
            order_link = tr_th.find('a', {'name': 'orderIdLinks'})
            if order_link:
                order['order_id'] = order_link.text.strip()
                order['order_url'] = order_link.get('href', '')

            # è®¢å•æ—¶é—´
            dealtime_span = tr_th.find('span', class_='dealtime')
            if dealtime_span:
                order['order_time'] = dealtime_span.get('title', '').strip()

            # åº—é“ºåç§°
            shop_span = tr_th.find('span', class_='order-shop')
            if shop_span:
                shop_link = shop_span.find('a', class_='shop-txt')
                if shop_link:
                    order['shop_name'] = shop_link.text.strip()

            # å•†å“ä¿¡æ¯
            tr_bd = tbody.find('tr', class_='tr-bd')
            if tr_bd:
                goods_item = tr_bd.find('div', class_='goods-item')
                if goods_item:
                    product_name = goods_item.find('a', class_='a-link')
                    if product_name:
                        order['product_name'] = product_name.get('title', '').strip()
                        order['product_url'] = "https:" + product_name.get('href', '')

                    # å•†å“æ•°é‡
                    goods_number = goods_item.find_next_sibling('div', class_='goods-number')
                    if goods_number:
                        quantity_text = goods_number.text.strip()
                        match = re.search(r'x(\d+)', quantity_text)
                        if match:
                            order['quantity'] = int(match.group(1))

                # æ”¶è´§äºº
                consignee_div = tr_bd.find('div', class_='consignee')
                if consignee_div:
                    consignee_span = consignee_div.find('span', class_='txt')
                    if consignee_span:
                        order['consignee'] = consignee_span.text.strip()

                    prompt_div = consignee_div.find('div', class_='prompt-01')
                    if prompt_div:
                        address_p = prompt_div.find('p')
                        if address_p:
                            order['address'] = address_p.text.strip()

                        phone_p = prompt_div.find_all('p')[-1]
                        if phone_p and '****' in phone_p.text:
                            order['phone'] = phone_p.text.strip()

                # è®¢å•é‡‘é¢
                amount_div = tr_bd.find('div', class_='amount')
                if amount_div:
                    amount_span = amount_div.find('span')
                    if amount_span:
                        amount_text = amount_span.text.strip()
                        match = re.search(r'[Â¥ï¿¥]?(\d+\.?\d*)', amount_text)
                        if match:
                            order['amount'] = float(match.group(1))

                    pay_span = amount_div.find('span', class_='ftx-13')
                    if pay_span:
                        order['payment_method'] = pay_span.text.strip()

                # è®¢å•çŠ¶æ€
                status_div = tr_bd.find('div', class_='status')
                if status_div:
                    status_span = status_div.find('span', class_='order-status')
                    if status_span:
                        order['status'] = status_span.text.strip()

            return order

        except Exception as e:
            print(f"è§£æå•ä¸ªè®¢å•æ—¶å‡ºé”™: {e}")
            return {}

    def crawl(self):
        """
        é‡å†™ crawl æ–¹æ³•ï¼Œæ”¯æŒå¤šé¡µçˆ¬å–ã€‚
        å› ä¸ºäº¬ä¸œè®¢å•æ˜¯åˆ†é¡µçš„ï¼Œæˆ‘ä»¬éœ€è¦åœ¨çˆ¬å–è¿‡ç¨‹ä¸­åŠ¨æ€åˆ¤æ–­æ˜¯å¦ç»§ç»­ã€‚
        """
        all_orders = []
        current_page = self.start_page

        while True:
            print(f"\n--- æ­£åœ¨è·å–ç¬¬ {current_page} é¡µè®¢å•æ•°æ® ---")

            # æ„é€ å½“å‰é¡µ URL
            url = self.base_url
            # æ›´æ–° referer
            headers = self.headers.copy()
            headers["referer"] = f"{self.base_url}?page={max(1, current_page - 1)}"
            params = {
                "page": current_page,
            }

            response = self.make_request(url, headers=headers, cookies=self.cookies,params = params)
            if not response:
                print("è¯·æ±‚å¤±è´¥ï¼Œåœæ­¢çˆ¬å–ã€‚")
                break

            # è§£æå½“å‰é¡µ
            page_orders = self.parse(response)
            if not page_orders:
                print("å½“å‰é¡µæ— è®¢å•æ•°æ®ï¼Œå¯èƒ½å·²çˆ¬å–å®Œæ¯•ã€‚")
                break

            all_orders.extend(page_orders)
            print(f"ç¬¬ {current_page} é¡µè·å–åˆ° {len(page_orders)} ä¸ªè®¢å•")

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç»“æŸé¡µ
            if self.end_page and current_page >= self.end_page:
                print(f"å·²è¾¾åˆ°æŒ‡å®šç»“æŸé¡µ {self.end_page}")
                break

            current_page += 1
            time.sleep(self.delay)  # å»¶è¿Ÿ

        # ä¿å­˜æ‰€æœ‰æ•°æ®
        if all_orders:
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_orders)} ä¸ªè®¢å•ã€‚")
            return all_orders

        else:
            print("\nâŒ çˆ¬å–ç»“æŸï¼Œæœªè·å–åˆ°ä»»ä½•è®¢å•æ•°æ®ã€‚")


# ------------------- ä½¿ç”¨ç¤ºä¾‹ -------------------

if __name__ == "__main__":

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = JdOrderSpider(
        cookies=cookie,
        start_page=1,
        end_page=1  # å¯é€‰ï¼šåªçˆ¬å‰3é¡µï¼›è®¾ä¸º None åˆ™çˆ¬åˆ°æœ«é¡µ
    )

    # å¼€å§‹çˆ¬å–
    spider.crawl()